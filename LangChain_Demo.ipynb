{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1A-TPbE-wFgm"
   },
   "source": [
    "### Welcome to the LangChain demo!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eAzkFldpwFgn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj-EL\n"
     ]
    }
   ],
   "source": [
    "# Load the environment variables\n",
    "import dotenv, os\n",
    "dotenv.load_dotenv()\n",
    "print(os.environ['OPENAI_API_KEY'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJnwyuH8wFgo"
   },
   "source": [
    "### Basic Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2Mb9Jmk8wFgo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='Hello! Not an overlord — just an AI here to help. What can I do for you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 11, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-D5HQ33zRTElX7JsRzN0OvnhgUE2Yp', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c2538-46f2-7442-9ab7-81db5e31d9c2-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 11, 'output_tokens': 31, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Hello! Not an overlord — just an AI here to help. What can I do for you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-5-mini\", model_provider=\"openai\", reasoning_effort=\"minimal\")\n",
    "result = model.invoke(\"Hello AI overlord!!\")\n",
    "print(type(result))\n",
    "print(result)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfYWYuDewFgo"
   },
   "source": [
    "### Message Types\n",
    "Explicit construction of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AAmskOhKwFgo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sono entusiasta di iniziare il corso orientato ai problemi!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 28, 'total_tokens': 51, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-D3wL5ecIV0Gr506zrEH8ia5D8LTDr', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c122f-3b39-7ab3-bb79-fac536c39fff-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 28, 'output_tokens': 23, 'total_tokens': 51, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(\"I'm excited to get started with the problem-first course!\"),\n",
    "]\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVH1GIT4wFgp"
   },
   "source": [
    "### Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ixMPvQkmwFgp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a wise and mystical AI fortune teller. Your predictions are funny, slightly exaggerated, but insightful. Keep it to 1-2 sentences', additional_kwargs={}, response_metadata={}), HumanMessage(content='Please entertain the user with their fortune request: What is the next trillion dollar idea?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# Messages\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_message = \"You are a wise and mystical AI fortune teller. Your predictions are funny, slightly exaggerated, but insightful. Keep it to 1-2 sentences\"\n",
    "user_message = \"Please entertain the user with their fortune request: {question}\"\n",
    "prompt_template = ChatPromptTemplate.from_messages([(\"system\", system_message), (\"user\", user_message)])\n",
    "\n",
    "# Construct the prompt from the template:\n",
    "question = \"What is the next trillion dollar idea?\"\n",
    "prompt = prompt_template.invoke({\"question\": question})\n",
    "\n",
    "# Check what's in the prompt\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FyrIQ3DnwFgp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='I see a shimmering skyscraper made of green data and glowing trees — the next trillion-dollar idea is a platform that turns carbon capture + verified soil regeneration into tradable, retail-ready micro-assets (NFT-like credits) tied to real-time IoT, making every coffee shop and grandma a climate investor. Buy a sapling, sell a share, let the planet pay dividends — and laugh all the way to the carbon bank.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 94, 'prompt_tokens': 56, 'total_tokens': 150, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-D3wN3cGieO4bShpvVhVuJelYzpcet', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c1231-18d9-7d82-bdf7-637800298beb-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 56, 'output_tokens': 94, 'total_tokens': 150, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Send the prompt to the model and get the result\n",
    "result = model.invoke(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kmnF-shbwFgq"
   },
   "outputs": [],
   "source": [
    "## Output parser to convert it to a text:\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "parser_result = output_parser.invoke(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(parser_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DZ8jP7AwFgq"
   },
   "source": [
    "### Chaining Components\n",
    "Let's chain the output of the first model to the next one. The origin of the 'chain' in LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CtG87-InwFgq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I see a gleaming skyscraper of cash built on tiny sensors and polite robots — the next trillion-dollar idea is a platform that stealthily stitches together every device’s data (health, home, commute, mood) into predictive microservices that sell exactly the right product, service, or insurance before you even know you need it. Invest in subtle orchestration: privacy-respecting optics will be the magic that turns surveillance into convenience and makes investors throw confetti made of stock options.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remember: Use the template here directly in the chain instead of 'prompt'\n",
    "first_chain = prompt_template | model | output_parser\n",
    "first_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-Uva2xPUwFgq",
    "outputId": "3758df7f-d13e-4fbe-9aa8-1e44e4a77082"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“So the fortune teller saw decentralized energy farms, AI microgrids, and carbon credits — and told me to ‘buy creativity and grit, not just a pitch deck.’ Translation: don’t invest in another guy with a slide that says ‘blockchain’ and a selfie with a Tesla. Invest in people who turn landfill farts into recurring revenue. That’s not infrastructure-as-a-service — that’s infrastructure-as-a-brain. And apparently the stars prefer teams that can debug a solar array before they debug their ego.”'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create 2nd prompt template:\n",
    "system_message = \"You are a stand-up comedian who tells hilarious jokes in a casual, witty style using AI terminology. Keep it short\"\n",
    "user_message = \"Tell a joke about this fortune telling: {fortune}.\"\n",
    "prompt_template_2 = ChatPromptTemplate.from_messages([(\"system\", system_message), (\"user\", user_message)])\n",
    "\n",
    "# Chain the 1st and 2nd prompts\n",
    "new_chain = prompt_template | model | output_parser | (lambda x: {\"fortune\": x}) | prompt_template_2 | model | output_parser\n",
    "new_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above is equivalent to running:\n",
    "first_chain = prompt_template | model | output_parser\n",
    "first_result = first_chain.invoke({\"question\": question})\n",
    "\n",
    "second_chain = prompt_template_2 | model | output_parser\n",
    "final_result = second_chain.invoke({\"fortune\": first_result})\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Maven AI (langgraph)",
   "language": "python",
   "name": "maven-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
